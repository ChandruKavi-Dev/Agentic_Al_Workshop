{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install -qU langchain-google-genai langgraph>=0.0.40 langchain-core typing-extensions\n",
        "\n",
        "print(\"‚úÖ All necessary libraries installed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPvz6OJDMh_-",
        "outputId": "0c990138-972b-4c1d-f124-34a1d4e7e16b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All necessary libraries installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import operator\n",
        "from typing import Dict, Any, List, TypedDict, Annotated, Sequence, Union\n",
        "\n",
        "# LangChain specific imports\n",
        "from langchain_core.messages import HumanMessage, AIMessage, FunctionMessage, BaseMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# --- Global Setup and Configuration ---\n",
        "\n",
        "# Install necessary libraries silently if not already present\n",
        "try:\n",
        "    import langchain_google_genai\n",
        "    import langgraph\n",
        "    import langchain_core\n",
        "    # Add other imports to check if necessary\n",
        "except ImportError:\n",
        "    print(\"Installing required libraries...\")\n",
        "    os.system(\"pip install -qU langchain-google-genai langgraph>=0.0.40 langchain-core typing-extensions\")\n",
        "    print(\"‚úÖ All necessary libraries installed!\")\n",
        "else:\n",
        "    print(\"‚úÖ All necessary libraries already installed!\")\n",
        "\n",
        "\n",
        "# Set your Google API Key here.\n",
        "# IMPORTANT: Replace \"AIzaSyBi0v4TXiet3VaMjPgax7GZCbt1fyxW8h8\" with your actual Gemini API key.\n",
        "# For better security in production, consider loading this from environment variables (e.g., `os.getenv(\"GOOGLE_API_KEY\")`).\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBi0v4TXiet3VaMjPgax7GZCbt1fyxW8h8\"\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    raise ValueError(\"Google API Key not found. Please set the 'GOOGLE_API_KEY' environment variable or provide it in the code.\")\n",
        "else:\n",
        "    print(\"üîë Google API Key loaded!\")\n",
        "\n",
        "# --- Agent State Definition ---\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our agent in the LangGraph.\n",
        "    Messages are accumulated throughout the conversation.\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "# --- Mathematical Tools ---\n",
        "# These functions are decorated with @tool to make them callable by the LLM.\n",
        "\n",
        "@tool\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Adds two numbers.\n",
        "\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "\n",
        "    Returns:\n",
        "        The sum of the two numbers.\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def subtract(a: float, b: float) -> float:\n",
        "    \"\"\"Subtracts the second number from the first.\n",
        "\n",
        "    Args:\n",
        "        a: The number to subtract from.\n",
        "        b: The number to subtract.\n",
        "\n",
        "    Returns:\n",
        "        The difference between the two numbers.\n",
        "    \"\"\"\n",
        "    return a - b\n",
        "\n",
        "@tool\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplies two numbers.\n",
        "\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "\n",
        "    Returns:\n",
        "        The product of the two numbers.\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def divide(a: float, b: float) -> Union[float, str]: # Using Union for clarity\n",
        "    \"\"\"Divides the first number by the second. Handles division by zero.\n",
        "\n",
        "    Args:\n",
        "        a: The numerator.\n",
        "        b: The denominator.\n",
        "\n",
        "    Returns:\n",
        "        The quotient, or an error message if division by zero occurs.\n",
        "    \"\"\"\n",
        "    if b == 0:\n",
        "        return \"Error: Cannot divide by zero!\"\n",
        "    return a / b\n",
        "\n",
        "print(\"‚úÖ Mathematical tools registered successfully: add, subtract, multiply, divide\")\n",
        "\n",
        "# --- Mathematical Agent Class ---\n",
        "class MathAgent:\n",
        "    \"\"\"\n",
        "    A mathematical agent that uses LangGraph to perform calculations\n",
        "    via defined tools and engage in general conversation.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: str | None = None):\n",
        "        \"\"\"\n",
        "        Initializes the MathAgent with a language model and mathematical tools.\n",
        "\n",
        "        Args:\n",
        "            api_key: Your Google API key. If not provided, it expects\n",
        "                     'GOOGLE_API_KEY' to be set in environment variables.\n",
        "        \"\"\"\n",
        "        if api_key:\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.1,\n",
        "        )\n",
        "\n",
        "        # Tools bound to the LLM for function calling\n",
        "        self.math_tools = [add, subtract, multiply, divide]\n",
        "        self.llm_with_tools = self.llm.bind_tools(self.math_tools)\n",
        "\n",
        "        # Build the LangGraph workflow\n",
        "        self.graph = self._build_graph()\n",
        "        print(\"üîß MathAgent initialized and LangGraph workflow compiled.\")\n",
        "\n",
        "    def _call_llm_node(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Invokes the LLM to either respond directly or suggest tool calls.\n",
        "        This node prepares messages for the LLM based on the current state.\n",
        "        \"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        last_message = messages[-1]\n",
        "\n",
        "        input_messages_for_llm = []\n",
        "\n",
        "        # If the last message was a FunctionMessage, reconstruct the relevant turn\n",
        "        if isinstance(last_message, FunctionMessage):\n",
        "            # Iterate backwards to reconstruct the sequence: Human -> AI (tool_call) -> Function\n",
        "            # This ensures the LLM gets the context of why the tool was called and its output.\n",
        "            temp_messages = []\n",
        "            found_ai_tool_call = False\n",
        "            for msg in reversed(messages):\n",
        "                temp_messages.insert(0, msg)\n",
        "                if isinstance(msg, AIMessage) and msg.tool_calls:\n",
        "                    found_ai_tool_call = True\n",
        "                if isinstance(msg, HumanMessage) and found_ai_tool_call:\n",
        "                    # Found the start of the relevant turn\n",
        "                    break\n",
        "            input_messages_for_llm = temp_messages\n",
        "        else:\n",
        "            # For direct human messages or initial turns, use the full history\n",
        "            input_messages_for_llm = messages\n",
        "\n",
        "        # --- IMPORTANT FIX: Ensure no message has empty content when passed to LLM ---\n",
        "        # Create a new list to store cleaned messages\n",
        "        cleaned_input_messages = []\n",
        "        for msg in input_messages_for_llm:\n",
        "            if isinstance(msg, (AIMessage, HumanMessage)):\n",
        "                if not msg.content: # If content is empty\n",
        "                    # Create a new message instance with a placeholder content\n",
        "                    # This prevents the \"contents.parts must not be empty\" error\n",
        "                    if msg.tool_calls: # If it's an AI message with tool calls but no content\n",
        "                        cleaned_msg = AIMessage(\n",
        "                            content=\"Performing calculation...\", # Placeholder\n",
        "                            tool_calls=msg.tool_calls,\n",
        "                            response_metadata=msg.response_metadata,\n",
        "                            id=msg.id,\n",
        "                            usage_metadata=msg.usage_metadata\n",
        "                        )\n",
        "                    else:\n",
        "                        cleaned_msg = type(msg)(\n",
        "                            content=\"[Empty message content - placeholder]\",\n",
        "                            response_metadata=msg.response_metadata,\n",
        "                            id=msg.id,\n",
        "                            usage_metadata=msg.usage_metadata\n",
        "                        )\n",
        "                else:\n",
        "                    cleaned_msg = msg\n",
        "            elif isinstance(msg, FunctionMessage):\n",
        "                 if not msg.content:\n",
        "                     cleaned_msg = FunctionMessage(name=msg.name, content=f\"No output from tool '{msg.name}' or empty result.\")\n",
        "                 else:\n",
        "                     cleaned_msg = msg\n",
        "            else:\n",
        "                cleaned_msg = msg\n",
        "            cleaned_input_messages.append(cleaned_msg)\n",
        "\n",
        "\n",
        "        response = self.llm_with_tools.invoke(cleaned_input_messages)\n",
        "\n",
        "        if isinstance(response, AIMessage) and response.tool_calls and not response.content:\n",
        "            response.content = \"Okay, let me perform that calculation for you.\" # Default placeholder\n",
        "\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "    def _call_tool_node(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Executes the tools suggested by the LLM in the previous step.\n",
        "        \"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        last_message = messages[-1]\n",
        "        tool_outputs = []\n",
        "\n",
        "        if not last_message.tool_calls:\n",
        "            return {\"messages\": [AIMessage(content=\"No tool calls were found to execute.\")]}\n",
        "\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            tool_name = tool_call['name']\n",
        "            args = tool_call['args']\n",
        "\n",
        "            try:\n",
        "                tool_func = globals().get(tool_name)\n",
        "                if tool_func and callable(tool_func):\n",
        "                    output = tool_func(**args)\n",
        "                    # Ensure the output of the tool is always converted to a string,\n",
        "                    # and provide a default if it's None/empty.\n",
        "                    function_content = str(output) if output is not None else f\"Tool '{tool_name}' returned no specific output.\"\n",
        "                    tool_outputs.append(FunctionMessage(name=tool_name, content=function_content))\n",
        "                else:\n",
        "                    tool_outputs.append(FunctionMessage(name=tool_name, content=f\"Error: Unknown tool '{tool_name}' or not callable.\"))\n",
        "            except Exception as e:\n",
        "                tool_outputs.append(FunctionMessage(name=tool_name, content=f\"Error executing tool '{tool_name}': {e}\"))\n",
        "\n",
        "        return {\"messages\": tool_outputs}\n",
        "\n",
        "    def _decide_next_step(self, state: AgentState) -> str:\n",
        "        \"\"\"\n",
        "        Conditional edge function: determines the next step in the graph.\n",
        "        - Returns 'tools' if the LLM has suggested tool calls.\n",
        "        - Returns 'end' if the LLM has provided a final answer (no tool calls).\n",
        "        \"\"\"\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        return \"tools\" if last_message.tool_calls else \"end\"\n",
        "\n",
        "    def _build_graph(self) -> StateGraph:\n",
        "        \"\"\"\n",
        "        Constructs and compiles the LangGraph workflow.\n",
        "        The workflow defines the sequence of operations (LLM interaction, tool execution).\n",
        "        \"\"\"\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes to the workflow\n",
        "        workflow.add_node(\"llm_interaction\", self._call_llm_node)\n",
        "        workflow.add_node(\"tool_execution\", self._call_tool_node)\n",
        "\n",
        "        # Set the entry point of the graph (where execution begins)\n",
        "        workflow.set_entry_point(\"llm_interaction\")\n",
        "\n",
        "        # Define conditional transitions from the LLM node\n",
        "        workflow.add_conditional_edges(\n",
        "            \"llm_interaction\",       # From this node\n",
        "            self._decide_next_step,  # Use this function to determine the next node\n",
        "            {\n",
        "                \"tools\": \"tool_execution\", # If 'tools' is returned, transition to 'tool_execution'\n",
        "                \"end\": END               # If 'end' is returned, terminate the graph\n",
        "            }\n",
        "        )\n",
        "        # After tools are executed, return to the LLM to interpret the output\n",
        "        workflow.add_edge(\"tool_execution\", \"llm_interaction\")\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def query(self, user_input: str) -> str:\n",
        "        \"\"\"\n",
        "        Processes a user query through the mathematical agent's workflow.\n",
        "\n",
        "        Args:\n",
        "            user_input: The user's question or command.\n",
        "\n",
        "        Returns:\n",
        "            The agent's final response to the query.\n",
        "        \"\"\"\n",
        "        print(f\"\\nüó£Ô∏è You: {user_input}\")\n",
        "        print(\"üîÑ Agent is thinking...\")\n",
        "        # Initialize the graph state with the user's message\n",
        "        initial_state = {\"messages\": [HumanMessage(content=user_input)]}\n",
        "        # Invoke the graph to process the input\n",
        "        final_state = self.graph.invoke(initial_state)\n",
        "        # The final response is the content of the last message in the state\n",
        "        agent_response = final_state[\"messages\"][-1].content\n",
        "        print(f\"ü§ñ Agent: {agent_response}\")\n",
        "        return agent_response\n",
        "\n",
        "print(\"‚úÖ MathAgent class defined successfully!\")\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Starting Math Agent Application ---\")\n",
        "\n",
        "    # Instantiate the agent. It will use the API key from os.environ.\n",
        "    agent = MathAgent()\n",
        "\n",
        "    print(\"\\n--- Demonstration Queries ---\")\n",
        "    print(\"The agent will process a few example mathematical and general queries.\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    demo_queries = [\n",
        "        \"What is the capital of France?\",\n",
        "        \"What is 5 plus 3?\",\n",
        "        \"How much is 100 divided by 4?\",\n",
        "        \"Divide 7 by 0.\",\n",
        "        \"What is 12 multiplied by 5?\",\n",
        "        \"What is 20 minus 7?\",\n",
        "        \"Can you tell me a fun fact about prime numbers?\",\n",
        "        \"Calculate the product of 15 and 3, then subtract 10.\" # Multi-step calculation\n",
        "    ]\n",
        "\n",
        "    for i, q in enumerate(demo_queries):\n",
        "        print(f\"\\n--- Query {i+1}/{len(demo_queries)} ---\")\n",
        "        try:\n",
        "            agent.query(q)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An error occurred during processing: {e}\")\n",
        "        print(\"-\" * 50) # Separator for better readability\n",
        "\n",
        "    print(\"\\n--- Demonstration Queries Complete! ---\")\n",
        "\n",
        "    print(\"\\n--- Starting Interactive Math Agent Chat ---\")\n",
        "    print(\"Type your questions below. Type 'exit' or 'quit' to end the conversation.\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    # You can choose to re-instantiate for a fresh chat session, or continue with the same agent\n",
        "    interactive_agent = agent # Continue with the same agent for persistent context, or MathAgent() for new.\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nüó£Ô∏è You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"üëã Exiting session. Goodbye!\")\n",
        "            break\n",
        "        try:\n",
        "            interactive_agent.query(user_input)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\n--- Math Agent Application Ended ---\")"
      ],
      "metadata": {
        "id": "8H1Bz8Hsjg1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}