{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai langgraph>=0.0.40 langchain-core typing-extensions\n",
        "\n",
        "import os\n",
        "import re\n",
        "from typing import Dict, Any, List, TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "\n",
        "# LangChain specific imports\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, FunctionMessage, BaseMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBi0v4TXiet3VaMjPgax7GZCbt1fyxW8h8\"\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our agent in the LangGraph.\n",
        "    Messages are accumulated throughout the conversation.\n",
        "    \"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "@tool\n",
        "def plus(a: float, b: float) -> float:\n",
        "    \"\"\"Adds two numbers together.\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "    Returns:\n",
        "        The sum of the two numbers.\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def subtract(a: float, b: float) -> float:\n",
        "    \"\"\"Subtracts the second number from the first number.\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "    Returns:\n",
        "        The difference between the two numbers.\n",
        "    \"\"\"\n",
        "    return a - b\n",
        "\n",
        "@tool\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplies two numbers together.\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "    Returns:\n",
        "        The product of the two numbers.\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def divide(a: float, b: float) -> float:\n",
        "    \"\"\"Divides the first number by the second number. Handles division by zero.\n",
        "    Args:\n",
        "        a: The numerator.\n",
        "        b: The denominator.\n",
        "    Returns:\n",
        "        The quotient, or an error message if division by zero occurs.\n",
        "    \"\"\"\n",
        "    if b == 0:\n",
        "        return \"Error: Cannot divide by zero!\"\n",
        "    return a / b\n",
        "\n",
        "print(\"‚úÖ Mathematical tools defined successfully!\")\n",
        "print(\"üîß Available tools: plus, subtract, multiply, divide\")\n",
        "\n",
        "class MathematicalAgent:\n",
        "    def __init__(self, api_key: str = None):\n",
        "        \"\"\"Initialize the Mathematical Agent with LLM and tools.\"\"\"\n",
        "        if api_key:\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.1,\n",
        "        )\n",
        "\n",
        "        self.math_tools = [plus, subtract, multiply, divide]\n",
        "        self.llm_with_tools = self.llm.bind_tools(self.math_tools)\n",
        "\n",
        "        # Build the graph\n",
        "        self.graph = self._build_graph()\n",
        "\n",
        "    def _call_llm_node(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Node to invoke the LLM for general questions or to decide on tool calls.\n",
        "        Explicitly structures messages when returning from a tool call.\n",
        "        \"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        last_message = messages[-1]\n",
        "\n",
        "\n",
        "        if isinstance(last_message, FunctionMessage):\n",
        "            # Find the preceding AIMessage with tool calls and the original HumanMessage\n",
        "            human_message = None\n",
        "            ai_tool_call_message = None\n",
        "            messages_for_llm = []\n",
        "\n",
        "            # Iterate backwards to find the relevant messages\n",
        "            for msg in reversed(messages):\n",
        "                if isinstance(msg, FunctionMessage):\n",
        "                    messages_for_llm.insert(0, msg) # Add function messages at the beginning of this segment\n",
        "                elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
        "                    ai_tool_call_message = msg\n",
        "                    messages_for_llm.insert(0, msg) # Add the AI tool call message\n",
        "                elif isinstance(msg, HumanMessage) and ai_tool_call_message is not None:\n",
        "                     # Stop when we find the human message that prompted the tool call\n",
        "                     # This assumes a simple turn structure (Human -> AI tool call -> Function)\n",
        "                     messages_for_llm.insert(0, msg)\n",
        "                     human_message = msg\n",
        "                     break\n",
        "                else:\n",
        "\n",
        "                     pass # For this specific issue, focusing on the last turn sequence\n",
        "\n",
        "            # If we found the necessary messages, use this structured list\n",
        "            if human_message and ai_tool_call_message and messages_for_llm:\n",
        "                 input_messages = messages_for_llm\n",
        "            else:\n",
        "                 # Fallback to using the full history if structure not found (e.g., initial turn)\n",
        "                 input_messages = messages\n",
        "        else:\n",
        "            # If the last message is not a FunctionMessage, use the full history\n",
        "            input_messages = messages\n",
        "\n",
        "\n",
        "        response = self.llm_with_tools.invoke(input_messages)\n",
        "\n",
        "        # --- START OF ORIGINAL FIX attempt ---\n",
        "        # If the LLM generated tool calls but its content is empty,\n",
        "        # add a placeholder content to avoid Gemini API's \"empty parts\" error\n",
        "        # (This fix was for a different potential empty content issue)\n",
        "        # This part might still be necessary or can be removed depending on the exact API behavior\n",
        "        if isinstance(response, AIMessage) and response.tool_calls and not response.content:\n",
        "            response_with_content = AIMessage(\n",
        "                content=\"Okay, let me perform that calculation for you.\",\n",
        "                tool_calls=response.tool_calls,\n",
        "                response_metadata=response.response_metadata,\n",
        "                id=response.id,\n",
        "                usage_metadata=response.usage_metadata\n",
        "            )\n",
        "            return {\"messages\": [response_with_content]}\n",
        "        else:\n",
        "            return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "    def _call_tool_node(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Node to execute tools invoked by the LLM.\n",
        "        \"\"\"\n",
        "        messages = state[\"messages\"]\n",
        "        last_message = messages[-1]\n",
        "\n",
        "        tool_outputs = []\n",
        "        if last_message.tool_calls: # Ensure there are tool calls to process\n",
        "            for tool_call in last_message.tool_calls:\n",
        "                tool_name = tool_call['name']\n",
        "                args = tool_call['args']\n",
        "\n",
        "                try:\n",
        "                    # Dynamically call the tool function based on its name\n",
        "                    # This makes it more robust if you add more tools\n",
        "                    tool_func = globals().get(tool_name)\n",
        "                    if tool_func and callable(tool_func):\n",
        "                        output = tool_func(**args)\n",
        "                    else:\n",
        "                        output = f\"Error: Unknown tool '{tool_name}' or not callable.\"\n",
        "                    tool_outputs.append(FunctionMessage(name=tool_name, content=str(output)))\n",
        "                except Exception as e:\n",
        "                    tool_outputs.append(FunctionMessage(name=tool_name, content=f\"Error executing tool '{tool_name}': {str(e)}\"))\n",
        "        else:\n",
        "            # This case should ideally not be hit if graph logic is correct,\n",
        "            # but as a fallback, send a message indicating no tool calls were found.\n",
        "            tool_outputs.append(AIMessage(content=\"No tool calls were found in the last message to execute.\"))\n",
        "\n",
        "        return {\"messages\": tool_outputs}\n",
        "\n",
        "    def _decide_next_step(self, state: AgentState) -> str:\n",
        "        \"\"\"\n",
        "        Conditional edge to decide the next step:\n",
        "        - 'tools' if the LLM suggested tool calls.\n",
        "        - 'end' if the LLM provided a final answer.\n",
        "        \"\"\"\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        if last_message.tool_calls:\n",
        "            return \"tools\"\n",
        "        else:\n",
        "            return \"end\"\n",
        "\n",
        "    def _build_graph(self) -> StateGraph:\n",
        "        \"\"\"Build the LangGraph workflow.\"\"\"\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes\n",
        "        workflow.add_node(\"llm\", self._call_llm_node)\n",
        "        workflow.add_node(\"tools\", self._call_tool_node)\n",
        "\n",
        "        # Set the entry point\n",
        "        workflow.set_entry_point(\"llm\")\n",
        "\n",
        "        # Define conditional edges\n",
        "        workflow.add_conditional_edges(\n",
        "            \"llm\",          # From the 'llm' node\n",
        "            self._decide_next_step, # Use this function to decide the next step\n",
        "            {\n",
        "                \"tools\": \"tools\", # If 'tools' is returned, go to the 'tools' node\n",
        "                \"end\": END        # If 'end' is returned, terminate the graph\n",
        "            }\n",
        "        )\n",
        "        # After tools are executed, go back to the LLM to interpret the tool output\n",
        "        workflow.add_edge(\"tools\", \"llm\")\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def query(self, user_input: str) -> str:\n",
        "        \"\"\"Process user query through the agent.\"\"\"\n",
        "        initial_state = {\"messages\": [HumanMessage(content=user_input)]}\n",
        "        final_state = self.graph.invoke(initial_state)\n",
        "        return final_state[\"messages\"][-1].content\n",
        "\n",
        "print(\"‚úÖ MathematicalAgent class defined successfully!\")\n",
        "\n",
        "\n",
        "# --- Direct Interaction in Code ---\n",
        "# 1. Instantiate the agent with your API key\n",
        "# Make sure to replace \"YOUR_GEMINI_API_KEY\" with your actual key\n",
        "agent = MathematicalAgent(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "print(\"\\n--- Testing Agent Queries Directly in Code ---\")\n",
        "\n",
        "queries = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is 5 plus 3?\",\n",
        "    \"How much is 100 divided by 4?\",\n",
        "    \"Divide 7 by 0.\",\n",
        "    \"What is 12 multiplied by 5?\",\n",
        "    \"What is 20 minus 7?\"\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(f\"\\nYou: {q}\")\n",
        "    print(\"üîÑ Processing...\")\n",
        "    try:\n",
        "        response = agent.query(q)\n",
        "        print(f\"ü§ñ Agent: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during processing: {e}\")\n",
        "\n",
        "print(\"\\n--- All direct queries processed. ---\")"
      ],
      "metadata": {
        "id": "8H1Bz8Hsjg1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}